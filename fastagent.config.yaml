# FastAgent Configuration for Buddharauer
# This file configures FastAgent's LLM provider to use local Ollama models

# Generic OpenAI/Ollama Provider Configuration
# This provider uses Ollama's OpenAI-compatible API endpoint
generic:
  # Default API key for Ollama (not validated)
  api_key: "ollama"
  # Ollama's OpenAI-compatible endpoint  
  base_url: "http://localhost:11434/v1"

  # Model configurations
  models:
    # Main orchestrator and analyst model
    llama2:
      name: "llama2:latest"
      context_size: 4096
      temperature: 0.7
      max_tokens: 2048

    # Better tool calling model
    qwen:
      name: "qwen:latest"
      context_size: 8192
      temperature: 0.5
      max_tokens: 2048

    # Web search and summarization
    mistral:
      name: "mistral:7b"
      context_size: 4096
      temperature: 0.3
      max_tokens: 1024

    # Embeddings model
    nomic:
      name: "nomic-embed-text"
      context_size: 8192
      temperature: 0.0
      max_tokens: 512

# Environment Variable Overrides (optional)
# You can override these settings using environment variables:
# - GENERIC_API_KEY: Override the API key
# - GENERIC_BASE_URL: Override the base URL (useful for remote Ollama instances)
# - GENERIC_MODEL_LLAMA: Override llama2 model name
# - GENERIC_MODEL_QWEN: Override qwen model name
# - GENERIC_MODEL_MISTRAL: Override mistral model name
# - GENERIC_MODEL_NOMIC: Override nomic model name
# To use a specific model with FastAgent, use the following format:
#   fast-agent --model generic.llama3.2:latest
#   fast-agent --model generic.qwen2.5:latest
#   fast-agent --model generic.mistral:7b

# Tested Models for Tool Calling
# FastAgent has officially tested these models for tool calling and structured generation:
# - generic.llama3.2:latest (✅ Recommended for orchestrator and analyst)
# - generic.qwen2.5:latest (✅ Recommended for retrieval and analysis)
#
# Other models may work but are not officially tested:
# - generic.mistral:7b (⚠️ Limited testing for web search)
# - generic.phi3:mini (⚠️ Limited testing for lightweight tasks)

# Notes:
# 1. Make sure Ollama is running before using FastAgent: `ollama serve`
# 2. Pull required models first: `ollama pull llama3.2:latest`
# 3. Test connectivity: `fast-agent --model generic.llama3.2:latest`
# 4. For programmatic use in FastAPI, set environment variables or use this config

# Integration with FastAPI
# In your FastAPI application (src/api/main.py):
#   import os
#   os.environ["GENERIC_API_KEY"] = "ollama"
#   os.environ["GENERIC_BASE_URL"] = "http://localhost:11434/v1"
#
#   from fastagent import Agent
#   orchestrator = Agent(
#       name="orchestrator",
#       model="generic.llama3.2:latest",
#       system_prompt="You are a helpful document Q&A assistant..."
#   )
