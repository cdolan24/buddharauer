# Buddharauer - AI-Powered PDF Analysis System

An intelligent document analysis application with a chat-based interface that enables semantic search, entity extraction, and interactive querying of PDF documents using **local AI models**.

## Overview

Buddharauer processes large PDF documents, extracts structured information (characters, locations, items), and provides an AI-powered chat interface with side-by-side source document viewing. Built with local LLMs (via Ollama), it runs entirely on your infrastructure with no cloud dependencies.

## âœ¨ Key Features

### ğŸ¤– Chat-Based Interface with Split Screen
- **Chat Window**: Natural conversation with AI agents
- **Document Viewer**: Side-by-side source document display
- **Live Citations**: Highlighted passages with page numbers
- **Context Preservation**: Multi-turn conversations with memory

### ğŸ“š Document Processing Pipeline
- Extract text and images from large PDFs (1000+ pages supported)
- Intelligent chunking with semantic awareness
- Generate vector embeddings for semantic search
- Support for adding new documents retroactively

### ğŸ¯ Multi-Agent System
- **Orchestrator Agent**: Routes user questions to specialized agents
- **Analyst Agent**: Summarizes data and provides creative insights
- **Web Search Agent**: Searches external sources when needed
- **Retrieval Agent**: Finds relevant chunks from vector database (RAG)

### ğŸ  Local-First Architecture
- **No Cloud Dependencies**: All models run locally via Ollama
- **Configurable Models**: Choose models per agent based on your hardware
- **Privacy**: Your documents never leave your machine
- **Cost-Effective**: No API fees

## Technology Stack

### Core Infrastructure
- **Agent Framework**: [FastAgent](https://fast-agent.ai/) (fast-agent-mcp v0.3.17+)
  - MCP-native agent orchestration
  - Tool calling and structured generation
- **Backend**: FastAPI (Python REST API wrapping FastAgent agents)
- **Frontend**: Gradio (interactive web UI)
- **Package Manager**: uv (preferred) or pip + venv
- **Python**: 3.13.5+ (required for FastAgent)

### Local AI/ML
- **LLM Server**: [Ollama](https://ollama.ai/) - Local model server
  - Recommended models: llama3.2, qwen2.5, mistral, phi
  - OpenAI-compatible API (localhost:11434/v1)
  - Configurable model locations
- **FastAgent Integration**: Uses generic provider to connect to Ollama
  - Configuration via fastagent.config.yaml
  - Model specification: `generic.model_name:tag`

### Vector Database
- **ChromaDB** (recommended for MVP): Local, Python-native
- **Qdrant** (production): Higher performance, Docker deployment

### PDF Processing
- **PyMuPDF (fitz)**: Text + image extraction
- **Pillow**: Image processing
- **pytesseract** (optional): OCR for image-based text

### Additional Components
- **SQLite**: Metadata, user management, query logs
- **pytest**: Testing framework
- **bcrypt** (optional): Password hashing for authentication

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Gradio Web Interface                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Chat Window    â”‚      â”‚  Document Viewer       â”‚     â”‚
â”‚   â”‚  (User Q&A)     â”‚      â”‚  (Source PDFs/Markdown)â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/REST API
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FastAPI Backend                            â”‚
â”‚   /api/chat, /api/documents, /api/search, /api/health      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             FastAgent Orchestration Layer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Orchestrator  â”‚ â”‚ Analyst  â”‚ â”‚Web Search  â”‚ â”‚Retrievalâ”‚ â”‚
â”‚  â”‚ (FastAgent)  â”‚ â”‚(FastAgentâ”‚ â”‚ (FastAgent)â”‚ â”‚(FastAgentâ”‚ â”‚
â”‚  â”‚generic.llama â”‚ â”‚ sub-agentâ”‚ â”‚ +MCP tools)â”‚ â”‚ +RAG)   â”‚ â”‚
â”‚  â”‚3.2:latest    â”‚ â”‚)         â”‚ â”‚            â”‚ â”‚         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           FastAgent generic provider â†“                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama Server (localhost:11434) - OpenAI Compatible     â”‚
â”‚     Models: llama3.2, qwen2.5, mistral, phi, etc.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Storage                             â”‚
â”‚  [Vector DB]  [Documents]  [Images]  [SQLite Metadata]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Recommended Models by Agent (FastAgent + Ollama)

| Agent | FastAgent Model Spec | Ollama Model | Speed | Quality | RAM | FastAgent Tested |
|-------|---------------------|--------------|-------|---------|-----|------------------|
| **Orchestrator** | `generic.llama3.2:latest` | llama3.2 | Medium | High | 8GB | âœ… Yes |
| **Orchestrator** (alt) | `generic.qwen2.5:latest` | qwen2.5 | Medium | High | 7GB | âœ… Yes |
| **Analyst** | `generic.llama3.2:latest` | llama3.2 | Medium | High | 8GB | âœ… Yes |
| **Web Search** | `generic.mistral:7b` | mistral:7b | Fast | Medium | 6GB | âš ï¸ Limited |
| **Retrieval** | `generic.qwen2.5:latest` | qwen2.5 | Medium | High | 7GB | âœ… Yes |
| **Embeddings** | N/A (Ollama API) | nomic-embed-text | Fast | High | 2GB | N/A |

**Note**: FastAgent officially tests tool calling with `llama3.2:latest` and `qwen2.5:latest`. Other models may work but are not guaranteed.

**Alternative Models**:
- Low memory: `phi3:mini` (4GB), `mistral:7b` (6GB)
- High quality: `llama3:70b` (40GB), `qwen2:72b` (40GB)
- All models configurable in `fastagent.config.yaml` and `config.yaml`

## Hardware Requirements

### Minimum (Testing)
- **RAM**: 16GB
- **GPU**: Optional (CPU mode works)
- **Disk**: 20GB for models
- **Performance**: ~5-10s response time

### Recommended
- **RAM**: 32GB
- **GPU**: NVIDIA with 8GB+ VRAM (RTX 3070 or better)
- **Disk**: 50GB SSD
- **Performance**: ~2-3s response time

### Optimal
- **RAM**: 64GB
- **GPU**: NVIDIA with 16GB+ VRAM (RTX 4080/4090)
- **Disk**: 100GB+ NVMe SSD
- **Performance**: <1s response time

## ğŸš€ Quick Start

### 1. Install Prerequisites

```bash
# Install Python 3.13.5+ (required for FastAgent)
python --version

# Install uv (recommended) or use pip
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Ollama
# Visit: https://ollama.ai/download
# Or: curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models for FastAgent
ollama pull llama3.2:latest
ollama pull qwen2.5:latest
ollama pull mistral:7b
ollama pull nomic-embed-text
```

### 2. Clone & Setup

```bash
# Clone repository
git clone https://github.com/yourusername/buddharauer.git
cd buddharauer

# Create virtual environment
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
uv pip install -r requirements.txt

# Or with pip
pip install -r requirements.txt
```

### 3. Configure

```bash
# Setup FastAgent
fast-agent setup  # Creates fastagent.config.yaml

# Configure Ollama in fastagent.config.yaml:
# generic:
#   api_key: "ollama"
#   base_url: "http://localhost:11434/v1"

# Copy application config
cp config.example.yaml config.yaml

# Edit config.yaml to set:
# - FastAgent + Ollama integration
# - Model selections per agent (generic.model_name:tag)
# - Vector database settings
# - Chunking parameters
```

### 4. Add Documents

```bash
# Add PDFs to data/ directory
cp /path/to/your/documents/*.pdf data/

# Process documents
python scripts/process_documents.py
```

### 5. Start Application

```bash
# Start FastAPI backend (Terminal 1)
uvicorn src.api.main:app --reload --port 8000

# Start Gradio frontend (Terminal 2)
python src/frontend/app.py

# Or use the launcher script
python run.py
```

### 6. Access UI

Open browser to: **http://localhost:7860**

## ğŸ“ Project Structure

```
buddharauer/
â”œâ”€â”€ config.yaml              # Main configuration
â”œâ”€â”€ config.example.yaml      # Configuration template
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ run.py                   # Application launcher
â”‚
â”œâ”€â”€ data/                    # Raw PDF files (gitignored)
â”‚   â””â”€â”€ *.pdf
â”‚
â”œâ”€â”€ processed/               # Processed outputs (gitignored)
â”‚   â”œâ”€â”€ text/               # Extracted text
â”‚   â”œâ”€â”€ markdown/           # Converted markdown
â”‚   â”œâ”€â”€ metadata/           # Document metadata (JSON)
â”‚   â””â”€â”€ images/             # Extracted images
â”‚
â”œâ”€â”€ vector_db/              # ChromaDB/Qdrant data (gitignored)
â”‚
â”œâ”€â”€ data_storage/           # SQLite databases (gitignored)
â”‚   â”œâ”€â”€ documents.db       # Document registry
â”‚   â”œâ”€â”€ query_log.db       # Query history
â”‚   â””â”€â”€ users.db           # User accounts (optional)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/               # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ main.py       # API entry point
â”‚   â”‚   â”œâ”€â”€ routes/       # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py
â”‚   â”‚   â”‚   â””â”€â”€ search.py
â”‚   â”‚   â””â”€â”€ models/       # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/           # LangChain agents
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ analyst.py
â”‚   â”‚   â”œâ”€â”€ web_search.py
â”‚   â”‚   â””â”€â”€ retrieval.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/         # Document processing
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â””â”€â”€ image_processor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database/         # Database interfaces
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ document_registry.py
â”‚   â”‚   â””â”€â”€ query_logger.py
â”‚   â”‚
â”‚   â”œâ”€â”€ frontend/         # Gradio UI
â”‚   â”‚   â”œâ”€â”€ app.py       # Main Gradio app
â”‚   â”‚   â””â”€â”€ components/  # UI components
â”‚   â”‚
â”‚   â””â”€â”€ utils/           # Utilities
â”‚       â”œâ”€â”€ config.py    # Config loader
â”‚       â”œâ”€â”€ ollama_client.py
â”‚       â””â”€â”€ chunking.py  # Chunking strategies
â”‚
â”œâ”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ process_documents.py
â”‚   â”œâ”€â”€ setup_models.py
â”‚   â””â”€â”€ benchmark.py
â”‚
â””â”€â”€ specs/               # Detailed specifications
    â”œâ”€â”€ ARCHITECTURE_V2.md
    â”œâ”€â”€ API.md
    â”œâ”€â”€ IMPLEMENTATION_PLAN.md
    â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### fastagent.config.yaml (FastAgent LLM Provider)

```yaml
# FastAgent generic provider configuration for Ollama
generic:
  api_key: "ollama"  # Default for Ollama
  base_url: "http://localhost:11434/v1"  # Ollama OpenAI-compatible endpoint

# Optional: Override with environment variables
# GENERIC_API_KEY=ollama
# GENERIC_BASE_URL=http://localhost:11434/v1
```

### config.yaml (Application Configuration)

```yaml
# FastAgent + Ollama configuration
fastagent:
  provider: "generic"  # Use generic provider for Ollama
  ollama_base_url: "http://localhost:11434"
  models_path: "/path/to/ollama/models"  # Optional custom path

# Agent model selections (FastAgent model specs)
agents:
  orchestrator:
    model: "generic.llama3.2:latest"  # FastAgent model spec
    temperature: 0.7
    max_tokens: 2048

  analyst:
    model: "generic.llama3.2:latest"
    temperature: 0.5
    max_tokens: 4096

  web_search:
    model: "generic.mistral:7b"
    temperature: 0.3
    max_tokens: 1024

  retrieval:
    llm_model: "generic.qwen2.5:latest"  # For query reformulation
    embedding_model: "nomic-embed-text"  # Via Ollama embeddings API
    dimensions: 768

# Vector database
vector_db:
  type: "chromadb"  # or "qdrant"
  path: "./vector_db"

# Chunking for large PDFs
chunking:
  strategy: "semantic"  # semantic, fixed, recursive
  chunk_size: 800
  chunk_overlap: 150
  max_chunk_size: 1500
  min_chunk_size: 100

# API settings
api:
  host: "0.0.0.0"
  port: 8000

# Frontend settings
frontend:
  platform: "gradio"
  port: 7860
  theme: "soft"
  chat_history_length: 50
```

## ğŸ“– Usage Examples

### Chat Interface

```
User: "Who is Aragorn?"

[Orchestrator Agent]: *Searches vector database*
[Retrieval Agent]: *Finds relevant chunks from fellowship.pdf*

System: "Aragorn is a central character who appears throughout the documents.

IDENTITY & ROLE:
Aragorn is a ranger - a wandering warrior and protector. He is human
and male, with a hidden noble heritage that becomes important to the story.

KEY LOCATIONS:
- Bree: Where he first appears (pages 3, 15)
- Rivendell: An elven refuge (page 87)

[Source: fellowship.pdf, pages 3, 15, 42, 87]"
```

### Document Viewer

The right panel shows the source document with highlighted citations, allowing you to verify the AI's responses.

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test category
pytest tests/unit/
pytest tests/integration/
pytest tests/e2e/

# Test specific service
pytest tests/integration/test_api.py
pytest tests/unit/test_chunking.py
```

## ğŸ› ï¸ Development

### Adding New Models

```bash
# Pull new model
ollama pull qwen2:7b

# Update config.yaml
# Test with specific agent
python scripts/test_agent.py --agent analyst --model qwen2:7b
```

### Processing New Documents

```bash
# Add PDFs to data/
# Run processing script
python scripts/process_documents.py --watch

# Or process specific file
python scripts/process_documents.py --file data/newdoc.pdf
```

### API Development

```bash
# Start with auto-reload
uvicorn src.api.main:app --reload

# View API docs
open http://localhost:8000/docs
```

## ğŸ“Š Monitoring

### System Health

```bash
# Check API health
curl http://localhost:8000/api/health

# View metrics
curl http://localhost:8000/api/analytics/metrics
```

### Query Analytics

```bash
# Popular queries
curl http://localhost:8000/api/analytics/popular-queries

# System stats
curl http://localhost:8000/api/analytics/stats
```

## ğŸ—ºï¸ Roadmap

### Current Status: Planning Phase

See [Implementation Plan](specs/IMPLEMENTATION_PLAN.md) for detailed roadmap.

### Phase 0: Project Setup (Week 1)
- [x] Architecture design
- [ ] Development environment setup
- [ ] Ollama installation and model downloads
- [ ] Basic project structure

### Phase 1: Backend Core (Week 2-3)
- [ ] FastAPI setup with core endpoints
- [ ] Document processing pipeline
- [ ] Vector database integration
- [ ] LangChain agent framework

### Phase 2: Agents (Week 3-4)
- [ ] Orchestrator agent implementation
- [ ] Analyst agent implementation
- [ ] Web search agent
- [ ] RAG retrieval system

### Phase 3: Frontend (Week 4-5)
- [ ] Gradio chat interface
- [ ] Document viewer component
- [ ] Source citation display
- [ ] Chat history management

### Phase 4: Testing & Polish (Week 5-6)
- [ ] Comprehensive test suite
- [ ] Performance optimization
- [ ] Documentation completion
- [ ] Deployment guide

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

[Add license information]

## ğŸ™ Acknowledgments

- **Ollama** - Local LLM server
- **LangChain** - Agent orchestration framework
- **Gradio** - Web UI framework
- **ChromaDB** - Vector database

## ğŸ“š Documentation

Detailed specifications in `specs/` directory:
- [Architecture V2](specs/ARCHITECTURE_V2.md) - Complete system design
- [API Documentation](specs/API.md) - REST API reference
- [Implementation Plan](specs/IMPLEMENTATION_PLAN.md) - Development roadmap
- [User Stories](specs/user-stories-detailed.md) - Requirements

## Support

For issues and questions:
- **GitHub Issues**: [Report bugs or request features](https://github.com/yourusername/buddharauer/issues)
- **Documentation**: See `specs/` directory
- **Discussions**: [Community forum](https://github.com/yourusername/buddharauer/discussions)

---

**Note**: This is a local-first application. All processing happens on your machine. No cloud services or API keys required (except optional web search).
